{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0449a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import re\n",
    "import nltk\n",
    "import ast\n",
    "import string\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from datetime import datetime,timedelta\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476a257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Cleaning/Clean_Dataset.csv', encoding='utf-8')\n",
    "jakarta = pytz.timezone('Asia/Jakarta')\n",
    "time_date = \"%m/%d/%Y %H:%M\"\n",
    "df['datetime_created'] = df['Datetime'].apply(lambda x:datetime.strptime(x,time_date))\n",
    "df['date_created'] = df['datetime_created'].apply(lambda x:x.date())\n",
    "df['time_created'] = df['datetime_created'].apply(lambda x:x.time())\n",
    "df = df.drop(['datetime_created'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367ee452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41222"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9ec99",
   "metadata": {},
   "source": [
    "### Additional Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae4001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hapus duplikat\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3d308e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Clean_Text'].isnull() ==  True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7b9c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Text, dtype: object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[df['Clean_Text'].isnull() == True]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b6874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Clean_Text'])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f692a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             0\n",
       "Datetime                     0\n",
       "Tweet Id                     0\n",
       "Text                         0\n",
       "Username                     0\n",
       "Location                 24946\n",
       "Clean_Text                   0\n",
       "Unnamed: 7               39908\n",
       "old_indonlp_sentiment        0\n",
       "Lexicon_Sentiment            0\n",
       "Lexicon_Score                0\n",
       "Stop_Words_Text              0\n",
       "Stemmed_Text                 0\n",
       "language                     0\n",
       "Translated                   0\n",
       "label_score                  0\n",
       "score                        0\n",
       "date_created                 0\n",
       "time_created                 0\n",
       "Score_Scaled                 0\n",
       "indonlp_sentiment            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f716f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = df['Clean_Text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32fd49e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41207                                                                pejabat sini bodoh bicara era metaverse\n",
       "41208                                                                        mungkin cair kalau di metaverse\n",
       "41209                   walau terus merugi meta bakal lanjutkan proyek metaverse pada tahun berita teknologi\n",
       "41210    itu ciri kamu yang tidak bisa adaptasi di setiap ruang lingkup pasti ada kebiasan entah itu baik...\n",
       "41211    anggaran belanja untuk perangkat lunak tapi tidak bisa digunakan dengan maksimal ya buat apa pak...\n",
       "41212     dapat suntikkan us juta dari sbi dan square enix perusahaan game gumi bakal garap bisnis metaverse\n",
       "41213    saya tadinya ngira memang ada seperti orang dibalik ini semua yang ngatur mereka ke dunia lain t...\n",
       "41214                                                     punya dia lagi kembangkan ekosistem metaverse juga\n",
       "41215    wishnutama selaku founding chairman jagat nusantara optimis bahwa kehadiran metaverse tidak menu...\n",
       "41216    ekonomi saat ini tidak hanya bersifat fisik namun dalam dunia maya bisa menjadi nyata seperti me...\n",
       "41217                                                                    taunya beneran jual tanah metaverse\n",
       "41218               anak sama zaman sekarang ngadain webinar tentang metaverse dunia berlari secepat itu iya\n",
       "41219                                bicara saja metaverse pemilu masih pakai kardus sampah tidak berkembang\n",
       "41220               pesantren metaverse nanti untuk merangkul kayak fajar dikatakan pemegang kunci al markaz\n",
       "41221                                 bang bangun jangan keseringan mimpi di metaverse apalagi di metafisika\n",
       "Name: Clean_Text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "clean_text.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3e581",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d579bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gunakan slang dan stopwords untuk membersihkan dataset\n",
    "# sumber slang dan stop words : https://github.com/louisowen6/NLP_bahasa_resources\n",
    "def clean_tweets(text):\n",
    "    my_file = open('../Cleaning/cleaning_source/combined_stop_words.txt','r')\n",
    "    content = my_file.read()\n",
    "    stop_words = content.split('\\n')\n",
    "    file_2 = open('../Cleaning/cleaning_source/update_combined_slang_words.txt','r')\n",
    "    content2 = file_2.read()\n",
    "    slang_words = ast.literal_eval(content2)\n",
    "    my_file.close()\n",
    "    file_2.close()\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Remove url\n",
    "    text = re.sub(r'https?://[^\\s]+','',text)\n",
    "    # Remove hashtag\n",
    "    text = re.sub(r'#\\w+','',text)\n",
    "    \n",
    "    text = re.sub(r':', '', text)\n",
    "    text = re.sub(r'‚Ä¶', '', text)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    \n",
    "    #remove emojis from text\n",
    "    #text = emoji_pattern.sub(r'',text)\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = re.sub('[^a-zA-Z]',' ', text)\n",
    "    \n",
    "    #remove mentio\n",
    "    text=re.sub('&lt;/?.*?&gt;\",\"&lt;&gt;','',text)\n",
    "    \n",
    "    #remove digits and special chars\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    #remove other symbol from tweet\n",
    "    text = re.sub(r'â', '', text)\n",
    "    text = re.sub(r'€', '', text)\n",
    "    text = re.sub(r'¦', '', text)\n",
    "    \n",
    "    #modify slang word to make it more suitable\n",
    "    word_tokens = word_tokenize(text)\n",
    "    for w in word_tokens:\n",
    "        if w in slang_words.keys():\n",
    "            word_tokens[word_tokens.index(w)] = slang_words[w]\n",
    "    \n",
    "    #filter using NLTK and append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "    \n",
    "    #loop through condition\n",
    "    for w in word_tokens:\n",
    "        #check tokens against emoticons, punctuations and stopwords\n",
    "        if w not in string.punctuation and w not in stop_words:\n",
    "            filtered_tweet.append(w.lower())\n",
    "            \n",
    "    return ' '.join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b74a4b",
   "metadata": {},
   "source": [
    "### Word Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5126bb",
   "metadata": {},
   "source": [
    "# Import Lexicon Data\n",
    "##### sources:\n",
    "\n",
    "https://github.com/louisowen6/NLP_bahasa_resources\n",
    "https://github.com/abhimantramb/elang/blob/master/word2vec/utils/swear-words.txt\n",
    "https://github.com/fajri91/InSet\n",
    "https://github.com/agusmakmun/SentiStrengthID/blob/master/id_dict/sentimentword.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8cf0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "negasi = ['bukan','tidak','ga','gk','g', 'ngga', 'nggak', 'no']\n",
    "lexicon = pd.read_csv('Lexicon Dictionary/modified_full_lexicon.csv')\n",
    "lexicon = lexicon.drop(lexicon[(lexicon['word'] == 'bukan')|\n",
    "                               (lexicon['word'] == 'tidak')|\n",
    "                               (lexicon['word'] == 'ga')|\n",
    "                               (lexicon['word'] == 'gk')|\n",
    "                               (lexicon['word'] == 'ngga')|\n",
    "                               (lexicon['word'] == 'nggak')|\n",
    "                               (lexicon['word'] == 'no')|\n",
    "                               (lexicon['word'] == 'gk')].index,axis=0)\n",
    "lexicon = lexicon.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64c7b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_word = lexicon['word'].to_list()\n",
    "lexicon_num_words = lexicon['number_of_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80980f06",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a3917",
   "metadata": {},
   "source": [
    "#### Hitung nilai kata dengan mencocokkan dengan kamus lexicon sementara membuat matrix Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db0c9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sencol =[]\n",
    "senrow =np.array([])\n",
    "nsen = 0\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "sentiment_list = []\n",
    "# fungsi untuk menulis nilai kata jika ditemukan\n",
    "def found_word(ind,words,word,sen,sencol,sentiment,add):\n",
    "    # jika terdapat pada matrix Bag of Words, tingkatkan nilainya\n",
    "    if word in sencol:\n",
    "        sen[sencol.index(word)] += 1\n",
    "    else:\n",
    "    # jika tidak, tambahkan kata baru\n",
    "        sencol.append(word)\n",
    "        sen.append(1)\n",
    "        add += 1\n",
    "    # jika terdapat kata negasi sebelumnya, nilai sentimen akan menjadi negatif\n",
    "    if (words[ind-1] in negasi):\n",
    "        sentiment += -lexicon['weight'][lexicon_word.index(word)]\n",
    "    else:\n",
    "        sentiment += lexicon['weight'][lexicon_word.index(word)]\n",
    "    \n",
    "    return sen,sencol,sentiment,add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cb06125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memeriksa setiap kata, jika terdapat pada kamus lexicon, maka hitung nilai sentimennya\n",
    "for i in range(len(df)):\n",
    "    nsen = senrow.shape[0]\n",
    "    words = word_tokenize(df['Clean_Text'][i])\n",
    "    sentiment = 0 \n",
    "    add = 0\n",
    "    prev = [0 for ii in range(len(words))]\n",
    "    n_words = len(words)\n",
    "    if len(sencol)>0:\n",
    "        sen =[0 for j in range(len(sencol))]\n",
    "    else:\n",
    "        sen =[]\n",
    "    \n",
    "    for word in words:\n",
    "        ind = words.index(word)\n",
    "        # Memeriksa apakah terdapat pada kamus lexicon\n",
    "        if word in lexicon_word :\n",
    "            sen,sencol,sentiment,add= found_word(ind,words,word,sen,sencol,sentiment,add)\n",
    "        else:\n",
    "        # jika tidak, periksa kata dasarnya\n",
    "            kata_dasar = stemmer.stem(word)\n",
    "            if kata_dasar in lexicon_word:\n",
    "                sen,sencol,sentiment,add= found_word(ind,words,kata_dasar,sen,sencol,sentiment,add)\n",
    "        # jika masih tidak ditemukan, coba gabungkan dengan kata sebelumnya\n",
    "            elif(n_words>1):\n",
    "                if ind-1>-1:\n",
    "                    back_1    = words[ind-1]+' '+word\n",
    "                    if (back_1 in lexicon_word):\n",
    "                        sen,sencol,sentiment,add= found_word(ind,words,back_1,sen,sencol,sentiment,add)\n",
    "                    elif(ind-2>-1):\n",
    "                        back_2    = words[ind-2]+' '+back_1\n",
    "                        if back_2 in lexicon_word:\n",
    "                            sen,sencol,sentiment,add= found_word(ind,words,back_2,sen,sencol,sentiment,add)\n",
    "    # if there is new word founded, then expand the matrix\n",
    "    # jika ditemukan kata baru, maka perluas matrix\n",
    "    if add>0:  \n",
    "        if i>0:\n",
    "            if (nsen==0):\n",
    "                senrow = np.zeros([i,add],dtype=int)\n",
    "            elif(i!=nsen):\n",
    "                padding_h = np.zeros([nsen,add],dtype=int)\n",
    "                senrow = np.hstack((senrow,padding_h))\n",
    "                padding_v = np.zeros([(i-nsen),senrow.shape[1]],dtype=int)\n",
    "                senrow = np.vstack((senrow,padding_v))\n",
    "            else:\n",
    "                padding =np.zeros([nsen,add],dtype=int)\n",
    "                senrow = np.hstack((senrow,padding))\n",
    "            senrow = np.vstack((senrow,sen))\n",
    "        if i==0:\n",
    "            senrow = np.array(sen).reshape(1,len(sen))\n",
    "    # if there isn't then just update the old matrix\n",
    "    # jika tidak, perbarui matrix lama\n",
    "    elif(nsen>0):\n",
    "        senrow = np.vstack((senrow,sen))\n",
    "        \n",
    "    sentiment_list.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f59dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sencol.append('lexicon_sentiment')\n",
    "sentiment_array = np.array(sentiment_list).reshape(senrow.shape[0],1)\n",
    "sentiment_data = np.hstack((senrow,sentiment_array))\n",
    "df_sen = pd.DataFrame(sentiment_data,columns = sencol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1151f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b1b5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sen = df_sen.drop(df_sen.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecc4211e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kembang</th>\n",
       "      <th>terbaru</th>\n",
       "      <th>termasuk</th>\n",
       "      <th>perusahaan</th>\n",
       "      <th>dua</th>\n",
       "      <th>dunia</th>\n",
       "      <th>katanya</th>\n",
       "      <th>mau</th>\n",
       "      <th>down</th>\n",
       "      <th>melulu</th>\n",
       "      <th>...</th>\n",
       "      <th>fakboi</th>\n",
       "      <th>lebur</th>\n",
       "      <th>jotos</th>\n",
       "      <th>penghalang</th>\n",
       "      <th>terdampar</th>\n",
       "      <th>badung</th>\n",
       "      <th>suri</th>\n",
       "      <th>sembur</th>\n",
       "      <th>alim</th>\n",
       "      <th>lexicon_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   kembang  terbaru  termasuk  perusahaan  dua  dunia  katanya  mau  down  \\\n",
       "0        1        1         1           1    1      1        0    0     0   \n",
       "1        0        0         0           0    0      0        1    1     1   \n",
       "2        0        0         0           0    0      0        0    0     0   \n",
       "3        0        0         0           1    0      1        0    0     0   \n",
       "4        0        0         0           0    0      0        0    0     0   \n",
       "\n",
       "   melulu  ...  fakboi  lebur  jotos  penghalang  terdampar  badung  suri  \\\n",
       "0       0  ...       0      0      0           0          0       0     0   \n",
       "1       1  ...       0      0      0           0          0       0     0   \n",
       "2       0  ...       0      0      0           0          0       0     0   \n",
       "3       0  ...       0      0      0           0          0       0     0   \n",
       "4       0  ...       0      0      0           0          0       0     0   \n",
       "\n",
       "   sembur  alim  lexicon_sentiment  \n",
       "0       0     0                  9  \n",
       "1       0     0                 -2  \n",
       "2       0     0                 28  \n",
       "3       0     0                  8  \n",
       "4       0     0                 -2  \n",
       "\n",
       "[5 rows x 4140 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "674f5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "cek_df = pd.DataFrame([])\n",
    "cek_df['text'] = df['Clean_Text'].copy()\n",
    "cek_df['lexicon_sentiment'] = df_sen['lexicon_sentiment'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bc33588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lexicon_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cina dan as terus bersaing mengembangkan teknologi terbaru termasuk kali ini perusahaan di kedua...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katanya mau metaverse tapi down melulu bagaimana mas zuck</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mungkin facebook sudah berencana menjadi metaverse besar secara sudah menyiapkan diem sebagai st...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bersaing dengan perusahaan kelas dunia untuk mencapai hal ini sektor publik dan swasta harus ber...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sama sama semoga selalu terhindar dari usaha tipu menipu di metaverse iya</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>industri metaverse ini kenaikan sahamnya gede banget terbang so busines wise terlihat menjanjikan</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>industri metaverse memang sangat menjanjikan</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>perusahaan teknologi besar china sudah mengerjakan penawaran metaverse mereka</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dunia metaverse ini dikembangkan menggunakan teknologi blockchain dimana suatu infromasi data ak...</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>contoh beberapa aset tanah yang saya beli di dunia metaverse pavia seharga ada sekitar rupiah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  cina dan as terus bersaing mengembangkan teknologi terbaru termasuk kali ini perusahaan di kedua...   \n",
       "1                                            katanya mau metaverse tapi down melulu bagaimana mas zuck   \n",
       "2  mungkin facebook sudah berencana menjadi metaverse besar secara sudah menyiapkan diem sebagai st...   \n",
       "3  bersaing dengan perusahaan kelas dunia untuk mencapai hal ini sektor publik dan swasta harus ber...   \n",
       "4                            sama sama semoga selalu terhindar dari usaha tipu menipu di metaverse iya   \n",
       "5    industri metaverse ini kenaikan sahamnya gede banget terbang so busines wise terlihat menjanjikan   \n",
       "6                                                         industri metaverse memang sangat menjanjikan   \n",
       "7                        perusahaan teknologi besar china sudah mengerjakan penawaran metaverse mereka   \n",
       "8  dunia metaverse ini dikembangkan menggunakan teknologi blockchain dimana suatu infromasi data ak...   \n",
       "9        contoh beberapa aset tanah yang saya beli di dunia metaverse pavia seharga ada sekitar rupiah   \n",
       "\n",
       "   lexicon_sentiment  \n",
       "0                  9  \n",
       "1                 -2  \n",
       "2                 28  \n",
       "3                  8  \n",
       "4                 -2  \n",
       "5                 12  \n",
       "6                  9  \n",
       "7                  2  \n",
       "8                 -2  \n",
       "9                  2  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cek_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "e69f1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lexicon_Score'] = cek_df['lexicon_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "6155bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "-71\n"
     ]
    }
   ],
   "source": [
    "print(max(df['Lexicon_Score']))\n",
    "print(min(df['Lexicon_Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ec635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../Cleaning/Clean_Dataset.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
