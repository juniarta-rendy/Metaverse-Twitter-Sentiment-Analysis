{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1149cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "import ast\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import langid\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd3a6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "df = pd.read_csv('Clean_Dataset.csv',  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "343beb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#happy emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3' \n",
    "    ])\n",
    "#sad emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "#emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#combine happy and sad emoticon\n",
    "emoticons = emoticons_happy.union(emoticons_happy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a3b3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d89d066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open('cleaning_source/combined_stop_words.txt','r')\n",
    "content = my_file.read()\n",
    "stop_words = content.split('\\n')\n",
    "file_2 = open('cleaning_source/update_combined_slang_words.txt','r')\n",
    "content2 = file_2.read()\n",
    "slang_words = ast.literal_eval(content2)\n",
    "file_3 = open('cleaning_source/indonesia_dictionary.txt', 'r', encoding=\"utf-8\")\n",
    "content3 = file_3.read()\n",
    "indonesia_dictionary = content3.split('\\n')\n",
    "my_file.close()\n",
    "file_2.close()\n",
    "file_3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d3fb9",
   "metadata": {},
   "source": [
    "##### Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3da84fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use slang and stopwords to clean the dataset\n",
    "# slang and stop words source: https://github.com/louisowen6/NLP_bahasa_resources\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    # Remove url\n",
    "    text = re.sub(r'https?://[^\\s]+','',text)\n",
    "    # Remove hashtag\n",
    "    text = re.sub(r'#\\w+','',text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+','',text)\n",
    "    \n",
    "    # Remove word that containing number\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    \n",
    "    text = re.sub(r':', '', text)\n",
    "    text = re.sub(r'‚Ä¶', '', text)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    \n",
    "    #remove emojis from text\n",
    "    text = emoji_pattern.sub(r'',text)\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = re.sub('[^a-zA-Z]',' ', text)\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub('&lt;/?.*?&gt;\",\"&lt;&gt;','',text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #remove digits and special chars\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    #remove other symbol from tweet\n",
    "    text = re.sub(r'â', '', text)\n",
    "    text = re.sub(r'€', '', text)\n",
    "    text = re.sub(r'¦', '', text)\n",
    "    text = text.lower()\n",
    "    #remove repeated letter\n",
    "    return text\n",
    "    #translate and modify slang word to make it more suitable\n",
    "    '''\n",
    "    word_tokens = word_tokenize(text)\n",
    "    '''\n",
    "    for w in word_tokens:\n",
    "        if w in slang_words.keys():\n",
    "            word_tokens[word_tokens.index(w)] = slang_words[w]\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        if w not in indonesia_dictionary:\n",
    "            word_tokens[word_tokens.index(w)] = re.sub(r'(.)\\1+', r'\\1', w)\n",
    "    '''\n",
    "    #filter using NLTK and append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if w not in stop_words]\n",
    "    filtered_tweet = []\n",
    "    \n",
    "    #loop through condition\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        #check tokens against emoticons, punctuations and stopwords\n",
    "        if w not in emoticons and w not in string.punctuation and w not in stop_words:\n",
    "            filtered_tweet.append(w.lower())\n",
    "    \n",
    "    #stem the words\n",
    "    filtered_tweet = [stemmer.stem(word) for word in filtered_tweet]\n",
    "    \n",
    "    return ' '.join(filtered_tweet)\n",
    "    \n",
    "    #return ' '.join(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "132841f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stemmed_Text'] = df['Clean_Text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "23e56382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Clean_Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Text'].dropna(inplace=True)\n",
    "tokens = df['label'].apply(word_tokenize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
