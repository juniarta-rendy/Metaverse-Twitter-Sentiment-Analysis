{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06a1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = 'whitegrid')\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd894dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../Cleaning/Clean_Dataset.csv', encoding='UTF-8')\n",
    "df2 = pd.read_csv('../Additional_Dataset.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999ede44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42403, 18)\n",
      "(11509, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1959e72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juniarta\\AppData\\Local\\Temp\\ipykernel_31396\\1593761221.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df1.append(df2, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = df1.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8deea3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('../Cleaning/Combine_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ddc8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[['Clean_Text', 'indonlp_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d5601",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f382b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['Clean_Text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86bcea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cina dan as terus bersaing mengembangkan teknologi terbaru termasuk kali ini perusahaan di kedua negara berlomba lomba membangun dunia virtual atau metaverse', 'katanya mau metaverse tapi down melulu bagaimana mas zuck', 'mungkin facebook sudah berencana menjadi metaverse besar secara sudah menyiapkan diem sebagai stable curency mereka kalau emang iya pasti nanti pindah ke blokchain semua facebook masih menang banyak karena hanya memerlukan pembaruan teknologi penggunanya sudah banyak', 'bersaing dengan perusahaan kelas dunia untuk mencapai hal ini sektor publik dan swasta harus bersatu untuk membentuk ekosistem metaverse yang melibatkan teknologi ar vr dan xr dalam forum tersebut juga dibahas bagaimana mensukseskan platform ini dengan pasar konsumen', 'metaverse belum diadaptasi masif masih terus dikembangkan nyatanya hari ini kita kadang sudah bingung ujung pangkal dari persinggugangan realita empiris dan realita artifisial waw']\n"
     ]
    }
   ],
   "source": [
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d02a35",
   "metadata": {},
   "source": [
    "### Building a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e2d12",
   "metadata": {},
   "source": [
    "#### Tokenizing and Adding Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bde87a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   47   46    1]\n",
      " [   0    0    0 ...   85  291 2176]\n",
      " [   0    0    0 ... 2250   15   35]\n",
      " ...\n",
      " [   0    0    0 ...   54 2069  727]\n",
      " [   0    0    0 ...   23   42  819]\n",
      " [   0    0    0 ...   63   15 3938]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fc77f",
   "metadata": {},
   "source": [
    "#### Using BiDirectional and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea28742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
    "model2.add(layers.Dense(3,activation='softmax'))\n",
    "model2.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2b422",
   "metadata": {},
   "source": [
    "#### Label Encoding of Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3adbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "labels = np.array(train['indonlp_sentiment'])\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 'neutral':\n",
    "        y.append(0)\n",
    "    if labels[i] == 'negative':\n",
    "        y.append(1)\n",
    "    if labels[i] == 'positive':\n",
    "        y.append(2)\n",
    "y = np.array(y)\n",
    "labels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "750d14a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43129, 100) (43129, 3)\n",
      "(10783, 100) (10783, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b94146a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   47,   46,    1],\n",
       "       [   0,    0,    0, ...,   85,  291, 2176],\n",
       "       [   0,    0,    0, ..., 2250,   15,   35],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   54, 2069,  727],\n",
       "       [   0,    0,    0, ...,   23,   42,  819],\n",
       "       [   0,    0,    0, ...,   63,   15, 3938]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69e175",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b0350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 812/1079 [=====================>........] - ETA: 22s - loss: 0.6625 - accuracy: 0.7182"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X_train, y_train, epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e38f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d68af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment on data test by using model has been created, and then visualize a confusion matrix\n",
    "y_pred = np.argmax(model2.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print('Model Accuracy on Test Data:', accuracy)\n",
    "confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_true=y_true, y_pred=y_pred), fmt='g', annot=True)\n",
    "ax.xaxis.set_label_position('top')\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.set_xlabel('Prediction', fontsize=14)\n",
    "ax.set_xticklabels(['negative (0)', 'neutral (1)', 'positive (2)'])\n",
    "ax.set_ylabel('Actual', fontsize=14)\n",
    "ax.set_yticklabels(['negative (0)', 'neutral (1)', 'positive (2)'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eaff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# predict classes for test set\n",
    "y_pred = np.argmax(model2.predict(X_test), axis=-1)\n",
    "\n",
    "# convert one-hot encoded true labels to integer class labels\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "# calculate metrics\n",
    "print(classification_report(y_test_int, y_pred))\n",
    "\n",
    "# calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test_int, y_pred)\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e58b00",
   "metadata": {},
   "source": [
    "#### Prediction and Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = ['Neutral', 'Negative', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['metaverse teknologi yang sangat buruk'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model2.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2db602",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['wow metaverse teknologi yang sangat menjanjikan'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model2.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16349f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['kita lihat saja bagaimana perkembangan teknologi tersebut'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model2.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31324d73",
   "metadata": {},
   "source": [
    "#### Building Model again with some Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Bidirectional(layers.LSTM(15,dropout=0.6)))\n",
    "model.add(layers.Dense(3,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(tweets, labels, epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a82167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f117e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842352ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment on data test by using model has been created, and then visualize a confusion matrix\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print('Model Accuracy on Test Data:', accuracy)\n",
    "confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_true=y_true, y_pred=y_pred), fmt='g', annot=True)\n",
    "ax.xaxis.set_label_position('top')\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.set_xlabel('Prediction', fontsize=14)\n",
    "ax.set_xticklabels(['negative (0)', 'neutral (1)', 'positive (2)'])\n",
    "ax.set_ylabel('Actual', fontsize=14)\n",
    "ax.set_yticklabels(['negative (0)', 'neutral (1)', 'positive (2)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# predict classes for test set\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# convert one-hot encoded true labels to integer class labels\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "# calculate metrics\n",
    "print(classification_report(y_test_int, y_pred))\n",
    "\n",
    "# calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test_int, y_pred)\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['metaverse itu bahasa yahudinya aku cinta israel'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model2.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['itu kategori metaverse ya yang menang lalisa takes over vmas'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model2.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b86400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['teknologi yang sangat menjanjikan'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(model2.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2043e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be2033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
